
Logical misunderstandings: incorrect depiction of the operation (could try to quantify this a little better)
    0 points - Missed purpose of model entirely
    1 point - showed attempt at structuring code in a way that represents the given operation    
    2 points - Models the operation as intended, but misses one or a few things that were stated in the prompt.   
    3 points - Showed complete understanding of the prompt and created a system that modeled it effectively, regardless of syntaxial errors. For code with diagrams, diagram should make logical sense.
    
Syntax errors: Incorrect usage of brackets/parentheses or structural mistakes in placing a type of block under the wrong block 
    0 points: >.35 unique errors (errors are not unique if say, there is a bracket missing which results in an EOF error, but also results in a syntax error when a keyword is used correctly, but the block wasnâ€™t closed by a bracket so an error ensues)    
    1 point  <.35 unique errors  
    2 points <.25 (errors/line) unique errors     
    3 points <.05 unique errors
		
Mistaken prompt words for SysMLv2 terms, 
    0 points - Used a lot of language from the prompt (that is not SysMLv2 code) as keywords for sysmlv2,  >.10 (errors/lines) words from the prompt mistaken as sysmlv2	
    1 point - Used some language from the prompt as sysmlv2 keywords, (<.10 ratio)
    2 points - Used minimal language from the prompt as sysmlv2 keywords (<.05 ratio)
    3 points - Used exclusively sysmlv2 keywords and did not invent any code syntax


Note:
    I would definitely recommend providing an LLM with instructions
    and examples of SysMLv2 code for better outputs as in my case I 
    measured a very significant difference between the # errors with/without instructions, 
    currently just trying to feed LLM's as many prompts as possible, score the outputs, 
    and see if we can get effective diagrams. One thing I particularly noticed was that the 
    code output may be super readable, but GPT has no way of visualizing it and may make nonsensical 
    diagrams. GPT additionally writes actually code into some of the blocks instead of just things like states, 
    attributes, actions, etc. Currently I'm just mapping prompts/outputs to their respective scores based 
    on errors / lines onto a spreadsheet just to get a big enough data set for quantitative results.